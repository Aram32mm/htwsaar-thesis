%********************************************************************
% Appendix A â€“ System Configuration and Implementation Details
%*******************************************************
\chapter{System Configuration and Implementation Details}
\label{app:implementation}

This appendix documents the technical specifications of our hybrid retrieval system, providing essential details for deployment, configuration, and performance optimization.

\section{Configuration Parameters}
\label{sec:app-config}

\subsection{Core System Configuration}

The retrieval system operates with empirically tuned parameters discovered through Leave-One-Out Cross-Validation on our evaluation dataset.

\begin{table}[!ht]
\centering
\begin{tabular}{lll}
\toprule
\textbf{Parameter} & \textbf{Value} & \textbf{Purpose} \\
\midrule
\multicolumn{3}{l}{\textit{Retrieval Weights}} \\
\texttt{SEMANTIC\_WEIGHT} & 0.80 & Semantic similarity contribution \\
\texttt{BM25\_WEIGHT} & 0.10 & Keyword matching contribution \\
\texttt{FUZZY\_WEIGHT} & 0.10 & String similarity contribution \\
\midrule
\multicolumn{3}{l}{\textit{Operational Settings}} \\
\texttt{MIN\_SIMILARITY} & 0.30 & Relevance threshold ($\tau$) \\
\texttt{DEFAULT\_SEARCH\_LIMIT} & 10 & Results per query \\
\texttt{ENABLE\_RERANKING} & False & Metadata reranking \\
\midrule
\multicolumn{3}{l}{\textit{Model Configuration}} \\
\texttt{EMBEDDING\_MODEL} & UAE-Large-V1 & Transformer model \\
\bottomrule
\end{tabular}
\caption{System configuration with LOOCV-optimized parameters.}
\label{tab:config-params}
\end{table}

\subsection{Database Optimization}

SQLite configuration for read-heavy workloads:

\begin{lstlisting}[language=SQL, caption={SQLite optimization pragmas}, label={lst:db-pragmas}]
PRAGMA journal_mode = WAL;     -- Enable concurrent reads
PRAGMA synchronous = NORMAL;   -- Balance durability/speed
PRAGMA cache_size = -64000;    -- 64MB page cache
PRAGMA busy_timeout = 5000;    -- 5s timeout for locks
\end{lstlisting}

\section{Data Schema}
\label{sec:app-schema}

\subsection{Rule Representation}

Each validation rule comprises 15 fields organized into functional groups:

\begin{table}[!ht]
\centering
\small
\begin{tabular}{p{3.5cm}p{2.5cm}p{5cm}}
\toprule
\textbf{Field} & \textbf{Category} & \textbf{Purpose} \\
\midrule
\texttt{rule\_name} & Identifier & Primary human-readable key \\
\texttt{rule\_code} & Implementation & Kotlin validation logic \\
\texttt{llm\_description} & Search & Enhanced semantic content \\
\texttt{keywords} & Search & BM25 retrieval terms \\
\texttt{embedding} & Search & 1024-d semantic vector \\
\texttt{rule\_type} & Metadata & Categorical classification \\
\texttt{country} & Metadata & Geographic scope \\
\texttt{business\_type} & Metadata & Business domain \\
\texttt{party\_agent} & Metadata & Party relationships \\
\texttt{bansta\_error\_code} & Reference & Banking standard code \\
\texttt{iso\_error\_code} & Reference & ISO 20022 code \\
\bottomrule
\end{tabular}
\caption{Essential fields in the standardized corpus schema.}
\label{tab:schema-fields}
\end{table}

\section{API Reference}
\label{sec:app-api}

\subsection{Primary Search Interface}

\begin{lstlisting}[language=Python, caption={Search API specification}, label={lst:search-api}]
def search_rules(
    query: Optional[str] = None,
    rule_type: Optional[List[str]] = None,
    country: Optional[List[str]] = None,
    business_type: Optional[List[str]] = None,
    party_agent: Optional[List[str]] = None,
    mode: SearchMode = SearchMode.HYBRID,
    top_k: int = 10
) -> List[Dict[str, Any]]:
    """
    Search validation rules with optional filters.
    
    Returns:
        List of rules with 'search_score' field added.
        Rules are sorted by relevance (highest first).
    """
\end{lstlisting}

\subsection{Search Modes}

The system supports four retrieval strategies:

\begin{itemize}[leftmargin=*,itemsep=2pt,topsep=2pt]
  \item \texttt{HYBRID}: Weighted combination of all signals (default)
  \item \texttt{SEMANTIC}: Embedding similarity only
  \item \texttt{KEYWORD}: BM25 retrieval only  
  \item \texttt{FUZZY}: String matching only
\end{itemize}

\section{Deployment Guide}
\label{sec:app-deployment}

\subsection{Development Environment}

\begin{lstlisting}[language=bash, caption={Quick start}, label={lst:quickstart}]
# Setup
python -m venv env
source env/bin/activate
pip install -r requirements.txt

# Launch
python app.py
# Access at http://localhost:8050
\end{lstlisting}

\subsection{Production Deployment}

For production use, deploy with gunicorn behind a reverse proxy:

\begin{lstlisting}[language=bash, caption={Production configuration}, label={lst:production}]
gunicorn app:server \
    --workers 4 \
    --worker-class sync \
    --bind 0.0.0.0:8050 \
    --timeout 120 \
    --preload
\end{lstlisting}

The \texttt{--preload} flag ensures indices are built once and shared across workers, reducing memory usage by 75\%.

\section{Performance Characteristics}
\label{sec:app-performance}

\subsection{Operational Metrics}

\begin{table}[!ht]
\centering
\begin{tabular}{lrr}
\toprule
\textbf{Metric} & \textbf{Value} & \textbf{Target} \\
\midrule
\multicolumn{3}{l}{\textit{System Resources}} \\
Memory footprint & 530 MB & < 1 GB \\
Startup time & 464 ms & < 1 s \\
Index build time & 252 ms & < 500 ms \\
\midrule
\multicolumn{3}{l}{\textit{Query Performance}} \\
Median latency & 58 ms & < 100 ms \\
P95 latency & 660 ms & < 1000 ms \\
P99 latency & 2.1 s & < 3 s \\
\midrule
\multicolumn{3}{l}{\textit{Retrieval Quality}} \\
MRR@5 & 48.2\% & > 40\% \\
Hit@5 & 80.8\% & > 75\% \\
\bottomrule
\end{tabular}
\caption{System performance against design targets.}
\label{tab:performance-summary}
\end{table}

\subsection{Latency Breakdown}

Query processing time varies by search mode and corpus filtering:

\begin{itemize}[leftmargin=*,itemsep=2pt,topsep=2pt]
  \item \textbf{Keyword mode}: 1.2ms (BM25 scoring only)
  \item \textbf{Semantic mode}: 2.4ms (cosine similarity)
  \item \textbf{Fuzzy mode}: 19.4ms (string matching)
  \item \textbf{Hybrid mode}: 58ms median (all signals combined)
\end{itemize}

Applying categorical filters reduces the candidate set, improving performance proportionally. With aggressive filtering (e.g., single country), hybrid search completes in under 10ms.

\section{Corpus Preparation}
\label{sec:app-pipeline}

\subsection{Data Pipeline}

The standardization pipeline transforms raw rule data through seven stages:

\begin{enumerate}[leftmargin=*,itemsep=2pt,topsep=2pt]
  \item \textbf{Consolidation}: Merge three source CSVs (1,743 rules)
  \item \textbf{Deduplication}: Remove duplicates by rule name (586 removed)
  \item \textbf{Enhancement}: Generate descriptions via Gemini-2.5-Pro
  \item \textbf{Extraction}: Derive keywords and categorical tags
  \item \textbf{Embedding}: Compute UAE-Large-V1 vectors
  \item \textbf{Normalization}: L2-normalize for cosine similarity
  \item \textbf{Serialization}: Export as single CSV (32.21 MB)
\end{enumerate}

\subsection{Embedding Generation}

Embeddings are generated offline using mean pooling over token representations:

\begin{lstlisting}[language=Python, caption={Embedding generation}, label={lst:embed-gen}]
# Load model
model = AutoModel.from_pretrained("UAE-Large-V1")
tokenizer = AutoTokenizer.from_pretrained("UAE-Large-V1")

# Generate embeddings
with torch.no_grad():
    inputs = tokenizer(text, return_tensors='pt', 
                      truncation=True, max_length=512)
    outputs = model(**inputs)
    
    # Mean pooling
    embeddings = outputs.last_hidden_state.mean(dim=1)
    
    # L2 normalization
    embeddings = F.normalize(embeddings, p=2, dim=1)
\end{lstlisting}

\section{Troubleshooting}
\label{sec:app-troubleshooting}

Common issues and solutions:

\begin{table}[!ht]
\centering
\small
\begin{tabular}{p{4cm}p{7cm}}
\toprule
\textbf{Issue} & \textbf{Resolution} \\
\midrule
High memory usage & Reduce workers; enable \texttt{--preload} \\
Slow startup & Pre-serialize indices to pickle files \\
Database locks & Ensure WAL mode; increase timeout \\
Poor recall & Verify embedding normalization \\
Inconsistent scores & Check per-query normalization \\
\bottomrule
\end{tabular}
\caption{Common operational issues and recommended solutions.}
\label{tab:troubleshooting}
\end{table}

\section{Summary}

This appendix provides the technical foundation for deploying and maintaining the hybrid retrieval system. The configuration parameters, optimized through empirical evaluation, balance retrieval quality with performance constraints. The modular API design enables integration with existing systems while maintaining the simplicity of our monolithic architecture. Organizations implementing similar systems should adjust parameters based on their specific corpus characteristics and performance requirements.
