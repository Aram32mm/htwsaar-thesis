\chapter{Conclusion}
\label{ch:conclusion}

This thesis presented a hybrid retrieval system for payment validation rules, demonstrating that practical information retrieval can be achieved within banking infrastructure constraints. Through careful engineering of proven techniques, we built a system that combines semantic embeddings, keyword matching, and string similarity to help quality assurance teams and developers find relevant rules quickly and accurately.

\section{Summary of Contributions}

\subsection{Technical Implementation}

We developed a retrieval system achieving 48.2\% MRR@5 through weighted combination of three complementary signals: semantic similarity (80\%), BM25 keyword matching (10\%), and fuzzy string matching (10\%). These weights, discovered through Leave-One-Out Cross-Validation on 30 test queries, improved performance by 4.8 percentage points (11\% relative improvement) over initial configurations. The system operates with median query latency of 58ms and uses 530MB of memory to serve 1,157 validation rules—well within our design targets of sub-100ms latency and under 1GB memory footprint.

The implementation deliberately chose standard Python libraries—Dash for the web interface, scikit-learn for similarity computation, and SQLite for data management—avoiding external dependencies that would complicate deployment in regulated environments. All embeddings are pre-computed offline using UAE-Large-V1 and stored as JSON strings within the CSV corpus, enabling single-file deployment while maintaining acceptable startup times (464ms). This architecture ensures complete reproducibility and auditability, essential requirements for banking systems.

\subsection{Data Standardization Achievement}

Beyond the retrieval system itself, a significant contribution lies in consolidating scattered rule documentation into a standardized, searchable corpus. This process transformed three inconsistent CSV sources containing 1,743 total rules into 1,157 unique, properly documented entries through:

\begin{itemize}[leftmargin=*,itemsep=2pt,topsep=2pt]
  \item Deduplication and schema alignment across heterogeneous formats
  \item Offline enrichment using Gemini-2.5-Pro to generate comprehensive descriptions and extract structured metadata
  \item Computation of 1024-dimensional embeddings enabling semantic search
  \item Establishment of sustainable update procedures maintaining data quality
\end{itemize}

The resulting corpus now serves as the single source of truth for validation rules, replacing multiple inconsistent documentation sources and providing a foundation for future enhancements.

\subsection{Methodological Rigor}

Despite limited evaluation data, we demonstrated rigorous methodology appropriate for specialized domains where large annotated datasets are unavailable. Leave-One-Out Cross-Validation maximized learning from 30 queries while avoiding overfitting. Our evaluation included ablation studies confirming each signal's contribution and sensitivity analysis showing robustness to ±10\% weight variations—critical for production systems where exact tuning may drift over time. This methodology provides a template for evaluating retrieval systems in data-scarce environments.

\section{Design Philosophy and Lessons Learned}

\subsection{The Power of Architectural Simplicity}

Our monolithic architecture—initially chosen due to deployment constraints—proved superior to distributed alternatives for our use case. Running all components in a single Python process eliminated network latency, simplified debugging, reduced deployment to a single container, and ensured the deterministic behavior required for audit compliance. For moderate-scale applications (1,157 rules, 10-20 concurrent users), this approach delivers better performance and maintainability than microservices architectures.

\subsection{Pragmatic Technology Choices}

Several decisions that might appear suboptimal in academic contexts proved correct for production deployment:

\begin{itemize}[leftmargin=*,itemsep=2pt,topsep=2pt]
  \item \textbf{CSV storage over databases}: Enables version control, manual inspection, and single-file deployment
  \item \textbf{JSON-encoded embeddings}: Maintains portability while preserving the simplicity of CSV format
  \item \textbf{Brute-force similarity search}: More reliable and faster than approximate methods at our scale
  \item \textbf{Avoiding FAISS}: Eliminated complexity without performance penalty for ~1,000 documents
\end{itemize}

These choices reflect a core principle: select the simplest tool that meets requirements, not the most sophisticated available. This philosophy enabled rapid development, straightforward deployment, and easy maintenance.

\section{Limitations and Boundaries}

\subsection{System Constraints}

We acknowledge clear boundaries that define the system's operational envelope:

\begin{itemize}[leftmargin=*,itemsep=2pt,topsep=2pt]
  \item \textbf{Scale ceiling}: Linear search complexity limits effectiveness beyond ~10,000 rules
  \item \textbf{Evaluation scope}: 30-query dataset provides preliminary validation but lacks statistical power
  \item \textbf{Language support}: Primarily English with limited German capability
  \item \textbf{Query sophistication}: No handling of typos, acronym expansion, or contextual inference
  \item \textbf{Update mechanism}: Corpus changes require service restart, acceptable for daily updates
\end{itemize}

\subsection{Known Failure Patterns}

Current implementation struggles with specific query patterns that represent opportunities for enhancement:

\begin{itemize}[leftmargin=*,itemsep=2pt,topsep=2pt]
  \item Mixed-language queries requiring multilingual understanding
  \item Temporal references requiring date-aware filtering
  \item Implicit context requiring organizational knowledge
  \item Domain-specific abbreviations absent from training data
\end{itemize}

These limitations are acceptable for initial deployment, with improvements prioritized based on actual usage patterns rather than speculation.

\section{Future Directions}

\subsection{Immediate Enhancements (3-6 months)}

Based on the current foundation, practical next steps focus on operational improvements:

\begin{itemize}[leftmargin=*,itemsep=2pt,topsep=2pt]
  \item \textbf{Evaluation expansion}: Collect 200+ real queries from production logs for statistically valid assessment
  \item \textbf{Usage analytics}: Implement query logging to understand actual search patterns and failure modes
  \item \textbf{Performance optimization}: Add caching for frequent queries and serialize indices to reduce startup time
  \item \textbf{Robustness improvements}: Enhanced error handling for malformed queries and edge cases
\end{itemize}

\subsection{System Evolution (6-12 months)}

With proven deployment and user feedback, the architecture supports natural extensions:

\begin{itemize}[leftmargin=*,itemsep=2pt,topsep=2pt]
  \item \textbf{Learning from usage}: Incorporate click-through data for relevance feedback
  \item \textbf{Multilingual expansion}: Deploy language-specific embeddings for German support
  \item \textbf{Integration capabilities}: Expose API endpoints for embedding in other tools
  \item \textbf{Operational efficiency}: Enable incremental updates without restart
\end{itemize}

\subsection{Research Opportunities}

While maintaining focus on practical deployment, several research directions could enhance retrieval quality if simpler approaches prove insufficient:

\begin{itemize}[leftmargin=*,itemsep=2pt,topsep=2pt]
  \item Learning-to-rank models leveraging user interaction data
  \item Cross-encoder architectures for precision reranking
  \item Domain-specific query expansion using banking terminology
  \item Privacy-preserving federated learning across institutions
\end{itemize}

\section{Concluding Remarks}

This thesis demonstrated that effective information retrieval for specialized domains requires neither cutting-edge models nor complex distributed architectures. By combining three well-understood retrieval signals through empirically tuned weights, standardizing scattered documentation, and implementing rigorous evaluation despite data constraints, we built a system that solves real problems within real constraints.

The key insight is that banking IT environments—with their stringent security requirements, audit demands, and deployment restrictions—benefit more from simple, auditable architectures than from sophisticated but opaque solutions. Our success validates the approach of applying proven techniques thoughtfully rather than pursuing algorithmic novelty.

As financial institutions manage growing regulatory complexity, tools for efficient rule discovery become critical infrastructure. This system provides a foundation for that capability, ready for production deployment at Deutsche Bank. Its successful adoption will validate our thesis that modern NLP capabilities can be delivered through pragmatic engineering, solving real problems for real users within the constraints that actually matter.

The hybrid retrieval system stands ready to transform how developers and QA teams interact with validation rules—reducing discovery time from hours to seconds while maintaining the reliability and auditability that banking demands. In choosing simplicity over sophistication, determinism over flexibility, and pragmatism over perfection, we built not just a research prototype but a production-ready tool that will deliver value from day one.
