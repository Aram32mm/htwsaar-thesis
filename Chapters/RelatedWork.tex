\chapter{Related Work}\label{ch:related-work}

This chapter positions our validation rule retrieval system within the broader landscape of information retrieval and financial compliance systems. We examine foundational work in lexical and semantic retrieval, trace the evolution of hybrid approaches, and identify critical gaps that motivate our contributions.

\section{Information Retrieval Foundations}

\subsection{Lexical Retrieval}

BM25 remains the dominant lexical retrieval method since its emergence from Robertson and Spärck Jones's probabilistic ranking framework \cite{robertson1976relevance,robertson1994okapi}. While Robertson et al. \cite{robertson2009bm25} provided theoretical justification, practical improvements remain elusive. Trotman et al. \cite{trotman2014improvements} found default parameters rarely optimal, motivating our empirical tuning to $k_1=1.5, b=0.3$ for keyword fields.

Our application of BM25 to curated keyword fields rather than full text aligns with Kim et al. \cite{kim2019structured}, who demonstrated superior performance on high-quality metadata. Unlike their focus on academic abstracts, we apply this principle to expert-curated regulatory keywords.

\subsection{Dense Retrieval}

Karpukhin et al.'s Dense Passage Retrieval \cite{karpukhin2020dense} established neural embeddings as viable alternatives to lexical methods. Reimers and Gurevych \cite{reimers2019sbert} made this practical through Sentence-BERT, enabling pre-computed embeddings with cosine similarity—the approach we adopt with UAE-Large-V1 \cite{uae2023large}.

While Johnson et al. \cite{johnson2019billion} popularized FAISS for large-scale similarity search, Douze et al. \cite{douze2024faiss} confirmed brute-force search remains optimal for corpora under 10,000 documents, validating our simpler approach.

\section{Hybrid Retrieval Systems}

\subsection{Combination Strategies}

Early hybrid work by Kuzi et al. \cite{kuzi2020hybrid} demonstrated complementary signals between lexical and semantic methods. Lin and Ma \cite{lin2021pretrained} formalized this, showing simple linear combinations often outperform complex fusion. Their convex combination approach inspired our three-weight system.

For score fusion, we chose min-max normalization over alternatives like reciprocal rank fusion \cite{cormack2009rrf} to preserve score magnitudes. Wang et al. \cite{wang2023improving} validated this approach for heterogeneous signals. While learning-to-rank methods exist \cite{liu2009learning,burges2005learning}, Qin et al. \cite{qin2021neural} found simple combinations competitive with limited training data—relevant given our 30 queries.

\subsection{Recent Neural Approaches}

ColBERT \cite{khattab2020colbert} and its successors represent sophisticated late-interaction models incompatible with our constraints. Generative retrieval approaches like DSI \cite{tay2022transformer} remain experimental and lack the interpretability required in banking.

\section{Financial Domain Applications}

\subsection{Regulatory Compliance Systems}

Arner et al. \cite{arner2017fintech} positioned "RegTech" as technology addressing regulatory challenges, identifying information retrieval as core. Hassan et al. \cite{hassan2022compliance} surveyed compliance systems, finding common limitations:
\begin{itemize}[leftmargin=*,itemsep=2pt,topsep=2pt]
 \item Reliance on keyword search despite poor recall
 \item Fragmentation across departments  
 \item Lack of semantic understanding
 \item Audit requirements constraining architecture
\end{itemize}

Our system addresses each through hybrid retrieval, corpus consolidation, semantic embeddings, and CSV-based audit trails.

\subsection{Code and Rule Retrieval}

While focused on rules rather than code analysis, insights from code search prove relevant. Husain et al. \cite{husain2019codesearch} showed natural language poorly matches code syntax—paralleling our challenge with rule descriptions. Gu et al. \cite{gu2018deepcode} combined multiple signals (method names, APIs, comments), inspiring our multi-signal approach.

Hay et al. \cite{hay2006defining} formalized business rule categories. Their taxonomy explains why retrieval works well for constraint rules (our focus): consistent structure amenable to embedding, unlike derivation or action rules.

\subsection{Existing Banking Solutions}

Commercial platforms (IBM OpenPages \cite{ibm2022openpages}, Thomson Reuters \cite{thomson2022regulatory}) excel at data management but lack semantic search and lightweight deployment. Academic prototypes \cite{zhang2020regulatory,zhong2021does} demonstrate neural approaches but require extensive training data and GPU infrastructure unavailable in our context.

\section{Technical Components}

\subsection{String Matching}

Our fuzzy matching builds on established edit distance work \cite{navarro2001guided,levenshtein1966}. The token-set ratio we employ \cite{fuzzywuzzy2011} proves robust to reordering—critical for rule names like "EUR Transfer Limit" vs "Limit Transfer EUR" \cite{cohen2003comparison}.

\subsection{Evaluation Methods}

Our LOOCV approach follows Kohavi \cite{kohavi1995cv} and Wong \cite{wong2015loocv}, who demonstrated its superiority for small datasets ($n < 50$). Metric selection (MRR@5, Hit@k) follows TREC standards \cite{voorhees1999trec}, with MRR appropriate for known-item search \cite{chapelle2009err}.

\subsection{Architecture Choices}

Our monolithic design contradicts microservices orthodoxy \cite{newman2015microservices} but aligns with recent findings. Bogner et al. \cite{bogner2019microservices} identified scenarios favoring monoliths: small teams, moderate scale, strict latency requirements, and heavy inter-component communication—all applicable here. Villamizar et al. \cite{villamizar2015evaluating} found monoliths use 30\% less CPU with 50\% lower latency, validating our choice.

\section{Critical Gaps and Our Contributions}

\subsection{Production Deployment Gaps}

While hybrid retrieval is well-studied, few papers detail production deployment under real constraints. Academic work assumes GPU availability, containerization, and external services—absent in banking. We demonstrate hybrid retrieval within these constraints.

Our CSV-first architecture storing embeddings as JSON strings appears nowhere in literature, yet solves real requirements: version control compatibility, audit trails, single-file deployment, and universal tool support.

\subsection{Domain-Specific Gaps}

No prior work addresses validation rule retrieval specifically. While code search is established, validation rules occupy a unique niche: more structured than general code, more complex than configuration, requiring both lexical and semantic understanding, and subject to regulatory audit.

Banking infrastructure constraints—no external APIs, no unapproved libraries, complete audit trails, deterministic behavior—are rarely acknowledged in academic literature, making most research irrelevant for our context.

\subsection{Methodological Contributions}

Most IR research assumes thousands of queries; our 30-query evaluation would be dismissed as insufficient. Yet this reflects specialized domains where expert annotations are expensive. We contribute rigorous evaluation methodology for data-scarce scenarios.

Our sensitivity analysis, varying weights by ±0.2 to reveal performance plateaus, provides crucial robustness information rarely reported but essential for production systems.

\section{Summary}

This review demonstrates our system's foundations in established techniques while highlighting novel contributions addressing unmet needs. We build upon decades of IR research while innovating within severe operational constraints unique to banking environments.

Our work challenges assumptions about complexity necessity, scale requirements, and architectural choices. By solving real problems with simple, auditable solutions, we demonstrate that practical IR systems need not sacrifice sophistication for deployability. As the field advances toward complex neural architectures and distributed systems, we provide evidence that simple solutions remain viable—even optimal—when they work within the constraints that matter.

\section{Comparative Analysis}
To crystallize the distinctions between existing approaches and our contributions, we present a systematic comparison across key operational dimensions.

Table~\ref{tab:related-work-comparison} synthesizes how our approach addresses limitations in existing solutions across key dimensions relevant to validation rule retrieval in banking environments.

\begin{table}[!htbp]
\centering
\small
\begin{tabular}{p{3cm}p{4cm}p{4cm}p{3.5cm}}
\toprule
\textbf{Dimension} & \textbf{Academic Solutions} & \textbf{Commercial Platforms} & \textbf{Our Approach} \\
\midrule
\textbf{Retrieval Method} & 
Pure neural (DPR, ColBERT) or pure lexical (BM25) & 
Keyword search with filters & 
Hybrid: BM25 + Fuzzy + Semantic with learned weights \\
\addlinespace

\textbf{Deployment Model} & 
Microservices, Docker, Kubernetes & 
Enterprise servers, distributed systems & 
Monolithic single-process Dash application \\
\addlinespace

\textbf{Storage Backend} & 
FAISS, Pinecone, Weaviate, PostgreSQL + pgvector & 
Oracle, SQL Server, proprietary databases & 
CSV with JSON embeddings + SQLite cache \\
\addlinespace

\textbf{Infrastructure Requirements} & 
GPUs, external APIs, cloud services & 
Dedicated servers, enterprise licenses & 
Single Python process, no external dependencies \\
\addlinespace

\textbf{Training Data Needs} & 
Thousands of labeled examples for fine-tuning & 
Not applicable (no learning) & 
30 queries sufficient via LOOCV \\
\addlinespace

\textbf{Semantic Understanding} & 
Advanced (fine-tuned models) but requires training & 
None or limited (taxonomy-based) & 
Pre-trained embeddings without fine-tuning \\
\addlinespace

\textbf{Audit Trail} & 
Often ignored or requires additional logging layer & 
Built-in but complex, database-centric & 
Native via CSV version control \\
\addlinespace

\textbf{Update Mechanism} & 
Retrain models, rebuild indices & 
Database transactions, ETL pipelines & 
Replace CSV, restart application \\
\addlinespace

\textbf{Latency} & 
Variable (10ms-10s depending on model) & 
Fast (<100ms) but keyword-only & 
Consistent (<1000ms P95) with semantic search \\
\addlinespace

\textbf{Evaluation Method} & 
Standard IR metrics on large test sets & 
User acceptance testing & 
LOOCV with sensitivity analysis \\
\bottomrule
\end{tabular}
\caption{Comparison of approaches across operational dimensions critical for banking deployment.}\label{tab:related-work-comparison}
\end{table}

This comparison reveals our system's unique position: combining the semantic capabilities of academic research with the operational simplicity required for banking deployment, while avoiding the complexity of commercial platforms. The key insight is that sophisticated retrieval need not require complex infrastructure when the corpus is moderate in size and the constraints are well-defined.

