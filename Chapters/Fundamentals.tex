\chapter{Fundamentals}
\label{ch:fundamentals}

This chapter introduces the theoretical foundations underpinning our hybrid retrieval system: lexical retrieval with BM25, fuzzy string matching, text embeddings and cosine similarity, the Retrieval-Augmented Generation (RAG) pattern, and monolithic architecture principles.

\section{Lexical Retrieval with BM25}

\subsection{Tokenization Theory}

Tokenization transforms continuous text into discrete units for statistical analysis. A token represents a meaningful text unit—typically a word—after preprocessing. The tokenization pipeline consists of:

\begin{enumerate}[leftmargin=*,itemsep=2pt,topsep=2pt]
\item \textbf{Segmentation}: Breaking text at word boundaries (whitespace, punctuation)
\item \textbf{Normalization}: Converting to canonical form (lowercase, expanding contractions)
\item \textbf{Filtering}: Removing stopwords or applying minimum frequency thresholds
\item \textbf{Stemming/Lemmatization}: Optional reduction to root forms
\end{enumerate}

Example transformation:
\begin{itemize}[leftmargin=*,itemsep=2pt,topsep=2pt]
\item Input: "EUR Cross-Border Payment Limits"
\item Tokens: ["eur", "cross", "border", "payment", "limits"]
\end{itemize}

Documents become bags of tokens—multisets where order is ignored but frequency matters. This representation enables statistical methods to measure relevance based on term occurrence patterns.

\subsection{The BM25 Ranking Function}

BM25 (Best Matching 25) extends the probabilistic relevance framework with term frequency saturation and document length normalization \cite{robertson2009bm25, robertson1994okapi}. For query $q$, document $d$, and query term $t \in q$:

\[
\mathrm{BM25}(q,d) = \sum_{t \in q} \mathrm{IDF}(t) \cdot \frac{f(t,d)\,(k_1 + 1)}{f(t,d) + k_1 \,\big(1 - b + b \cdot \frac{|d|}{\overline{|d|}}\big)}
\]

\paragraph{IDF Component.} Inverse document frequency quantifies term informativeness:
\[
\mathrm{IDF}(t) = \log\left(\frac{N - n(t) + 0.5}{n(t) + 0.5}\right)
\]

where $N$ is the corpus size and $n(t)$ counts documents containing $t$. The logarithm compresses the scale while the 0.5 smoothing prevents extreme values. Common terms receive low IDF scores; rare, discriminative terms receive high scores.

\paragraph{Term Frequency Saturation.} The term frequency component models diminishing returns:
\[
\frac{f(t,d)\,(k_1 + 1)}{f(t,d) + k_1 \,\big(1 - b + b \cdot \frac{|d|}{\overline{|d|}}\big)}
\]

As $f(t,d)$ increases, the contribution grows sub-linearly, approaching an asymptote at $(k_1 + 1)$. The parameter $k_1 \in [1.2, 2.0]$ controls saturation rate—higher values allow greater influence from repeated terms.

\paragraph{Length Normalization.} The term $b \cdot \frac{|d|}{\overline{|d|}}$ adjusts for document length bias. When $b=0$, no normalization occurs; when $b=1$, full normalization scales scores inversely with relative length. Typical values range from 0.3 to 0.75 depending on corpus characteristics.

\section{Fuzzy String Matching}

\subsection{Edit Distance Foundation}

Fuzzy matching quantifies similarity between non-identical strings using edit distance metrics \cite{navarro2001guided}. The Levenshtein distance counts minimum single-character edits (insertions, deletions, substitutions) to transform one string into another \cite{levenshtein1966}.

For strings $s_1$ and $s_2$, the normalized similarity ratio is:
\[
\text{ratio} = 1 - \frac{\text{LevenshteinDistance}(s_1, s_2)}{\max(|s_1|, |s_2|)}
\]

\subsection{Partial Matching Strategy}

Partial ratio matching finds the optimal substring alignment between query and target. Given query $q$ and target $t$ where $|q| \leq |t|$:

\begin{enumerate}[leftmargin=*,itemsep=2pt,topsep=2pt]
\item Find position $p$ in $t$ that minimizes edit distance to $q$
\item Compute similarity at optimal alignment
\item Return normalized score in [0, 1]
\end{enumerate}

This approach excels when:
\begin{itemize}[leftmargin=*,itemsep=2pt,topsep=2pt]
\item Query is a substring: "EUR limit" vs "EUR Daily Transfer Limit"
\item Query contains typos: "crossborder" vs "Cross-Border Payment"
\item Query is abbreviated: "SEPA max" vs "SEPA Maximum Amount"
\end{itemize}

\section{Dense Vector Representations}

\subsection{Distributional Semantics}

The distributional hypothesis states that words appearing in similar contexts have similar meanings \cite{harris1954distributional}. Modern embedding methods extend this principle through neural networks that learn dense vector representations preserving semantic relationships.

\subsection{Transformer-Based Encoding}

Transformer architectures \cite{vaswani2017attention} compute contextualized token representations through self-attention mechanisms. For input sequence $X = [x_1, ..., x_T]$:

\[
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
\]

where $Q$, $K$, $V$ are query, key, and value projections of the input. Multi-head attention applies this mechanism in parallel across different representation subspaces.

\subsection{Sentence-Level Embeddings}

To obtain fixed-size representations from variable-length text, we apply pooling over token embeddings. Mean pooling averages all token representations:

\[
\mathbf{e}_{\text{sentence}} = \frac{1}{T} \sum_{t=1}^{T} \mathbf{h}_t
\]

where $\mathbf{h}_t \in \mathbb{R}^{d}$ is the contextualized representation of token $t$. For our system, $d = 1024$ dimensions.

\subsection{Cosine Similarity}

Semantic similarity between vectors $\mathbf{q}$ and $\mathbf{d}$ is measured using cosine similarity:

\[
\cos(\mathbf{q}, \mathbf{d}) = \frac{\mathbf{q} \cdot \mathbf{d}}{\|\mathbf{q}\|_2 \, \|\mathbf{d}\|_2}
\]

For L2-normalized vectors where $\|\mathbf{v}\|_2 = 1$, this reduces to dot product:
\[
\cos(\mathbf{q}_{\text{norm}}, \mathbf{d}_{\text{norm}}) = \mathbf{q}_{\text{norm}} \cdot \mathbf{d}_{\text{norm}}
\]

This simplification enables efficient batch computation through matrix multiplication.

\section{Retrieval-Augmented Generation}

\subsection{RAG Architecture}

Retrieval-Augmented Generation combines parametric knowledge (encoded in model weights) with non-parametric knowledge (stored in external corpora) \cite{lewis2020rag}. The two-stage process consists of:

\begin{align}
\text{Retrieval}: & \quad \mathcal{D}_{\text{relevant}} = \text{Retrieve}(q, \mathcal{D}, k) \\
\text{Generation}: & \quad y = \text{Generate}(q, \mathcal{D}_{\text{relevant}})
\end{align}

\subsection{Offline-Online Separation}

Our implementation separates generative and retrieval phases temporally:

\paragraph{Offline Phase (Corpus Preparation):}
\begin{itemize}[leftmargin=*,itemsep=2pt,topsep=2pt]
\item Language models enrich rule descriptions
\item Keywords extracted from unstructured text
\item Categorical metadata inferred
\item All outputs validated and persisted
\end{itemize}

\paragraph{Online Phase (Query Processing):}
\begin{itemize}[leftmargin=*,itemsep=2pt,topsep=2pt]
\item Deterministic retrieval from pre-computed indices
\item Template-based explanation assembly
\item No model inference at query time
\item Complete audit trail maintained
\end{itemize}

This separation ensures reproducibility, auditability, and predictable latency—critical requirements in regulated environments.

\section{Hybrid Signal Combination}

\subsection{Score Normalization Theory}

Different retrieval signals operate on incomparable scales. BM25 produces unbounded positive scores, fuzzy matching yields percentages, and cosine similarity ranges from -1 to 1. Normalization enables meaningful combination.

\paragraph{Max Normalization.} We employ max normalization (also called max scaling):
\[
\widehat{s}_i = \frac{s_i}{\max_{r \in \mathcal{C}} s_i(r)}
\]

where $\mathcal{C}$ represents the candidate pool. This approach:
\begin{itemize}[leftmargin=*,itemsep=2pt,topsep=2pt]
\item Maps the highest score to 1.0
\item Preserves zero scores (maintaining sparsity)
\item Maintains relative proportions
\item Avoids issues with negative values
\end{itemize}

\subsection{Convex Combination}

Normalized scores combine through weighted averaging with constraints:
\[
s = \sum_{i} w_i \widehat{s}_i \quad \text{where} \quad \sum_{i} w_i = 1, \quad w_i \geq 0
\]

This convex combination ensures the final score remains in [0,1]. Our empirically tuned weights are:
\[
\boxed{ \; s = 0.80\,\widehat{s}_{\text{semantic}} + 0.10\,\widehat{s}_{\text{BM25}} + 0.10\,\widehat{s}_{\text{fuzzy}} \; }
\]

\subsection{Relevance Thresholding}

A minimum similarity threshold $\tau = 0.30$ filters low-confidence results:
\[
\text{include}(r) = \begin{cases}
\text{true} & \text{if } s(r) \ge \tau \\
\text{false} & \text{otherwise}
\end{cases}
\]

This threshold prevents spurious matches from appearing in results, improving precision at minimal recall cost.

\section{Monolithic Architecture Principles}

\subsection{Process Model}

A monolithic architecture consolidates all application components within a single operating system process. This contrasts with distributed architectures where components communicate across process or network boundaries.

\subsection{Memory Sharing Benefits}

Single-process execution enables:
\begin{itemize}[leftmargin=*,itemsep=2pt,topsep=2pt]
\item \textbf{Zero-copy data access}: Shared memory eliminates serialization
\item \textbf{Atomic operations}: No distributed coordination required
\item \textbf{Cache coherency}: Single address space ensures consistency
\item \textbf{Predictable latency}: No network round-trips
\end{itemize}

\subsection{Operational Simplicity}

Monolithic deployment simplifies:
\begin{itemize}[leftmargin=*,itemsep=2pt,topsep=2pt]
\item \textbf{Monitoring}: Single process metrics
\item \textbf{Debugging}: Unified stack traces
\item \textbf{Deployment}: One artifact to manage
\item \textbf{Versioning}: Atomic updates
\end{itemize}

\subsection{Trade-offs}

The monolithic approach trades horizontal scalability for simplicity. While microservices enable independent scaling of components, monoliths require vertical scaling (larger machines). For moderate-scale applications with stable load patterns, the operational benefits often outweigh scalability limitations.

\section{Performance Characteristics}

\subsection{Computational Complexity}

Index construction at startup:
\begin{itemize}[leftmargin=*,itemsep=2pt,topsep=2pt]
\item BM25: $O(N \cdot L)$ for $N$ documents, average length $L$
\item Embeddings: $O(N \cdot d)$ for $d$-dimensional vectors
\item Filters: $O(N \cdot T)$ for $T$ tags per document
\end{itemize}

Query-time retrieval over $C$ candidates:
\begin{itemize}[leftmargin=*,itemsep=2pt,topsep=2pt]
\item BM25: $O(|q| \cdot C)$ for query length $|q|$
\item Semantic: $O(d \cdot C)$ for vector dimension $d$
\item Fuzzy: $O(C \cdot M)$ for string length $M$
\item Combination: $O(C)$ for normalization and weighting
\end{itemize}

\subsection{Empirical Performance}

Leave-One-Out Cross-Validation on 30 annotated queries yielded:
\begin{itemize}[leftmargin=*,itemsep=2pt,topsep=2pt]
\item MRR@5 improved from 43.4\% (uniform weights) to 48.2\% (tuned)
\item Individual signals: BM25 38.8\%, Semantic 44.0\%, Fuzzy 24.2\%
\item Ablation confirmed complementary signal contributions
\item Sensitivity analysis showed robustness to $\pm$20\% weight perturbations
\end{itemize}

\section{Summary}

This chapter established the theoretical foundations of hybrid retrieval. BM25 provides principled lexical matching through probabilistic ranking. Fuzzy matching handles string variations via edit distance metrics. Dense embeddings capture semantic relationships through learned vector representations. Max normalization enables meaningful signal combination, while convex weighting ensures valid score ranges. The monolithic architecture leverages shared memory for efficiency while maintaining operational simplicity. Together, these fundamentals form a retrieval system that balances theoretical rigor with practical constraints.
